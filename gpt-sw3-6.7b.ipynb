{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "355db140-df73-43f6-ae7a-eec63d1f868b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Notebook by Daniel Zakrisson at Scaleout - www.scaleoutsystems.com\n",
    "# \n",
    "# This process will load large models (i.e. several .bin files), small models has \n",
    "# another process (see separate example).\n",
    "# \n",
    "# Assumes models are downloaded and available in a sub directory, \n",
    "# e.g. using 'git clone https://huggingface.co/AI-Sweden-Models/gpt-sw3-6.7b-v2'\n",
    "# \n",
    "# There also needs to be a sub folder named \"offload\" if the device map (see below) uses any disk.\n",
    "\n",
    "import torch\n",
    "from transformers import pipeline, AutoModelForCausalLM\n",
    "\n",
    "# Initialize GPU\n",
    "device = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "# Empty GPU cache and check how much GPU memory is already allocated (by the CUDA cores)\n",
    "torch.cuda.empty_cache()\n",
    "torch.ones(1).cuda()\n",
    "print(torch.cuda.memory_reserved(0)) \n",
    "print(torch.cuda.memory_allocated(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdd3f93a-04f5-4632-aa8c-0b7b8ada4f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model initiation is done differently than with smaller models, \n",
    "# explicitly initialize empty weights and then tie the weights to avoid \n",
    "# loading all weights twice (double memory usage). Accelerate is used to split model across devices.\n",
    "\n",
    "from transformers import AutoConfig\n",
    "from accelerate import init_empty_weights, dispatch_model, infer_auto_device_map\n",
    "\n",
    "checkpoint = 'gpt-sw3-6.7b-v2'\n",
    "config = AutoConfig.from_pretrained(checkpoint)\n",
    "\n",
    "with init_empty_weights():\n",
    "    model = AutoModelForCausalLM.from_config(config)\n",
    "model.tie_weights()\n",
    "\n",
    "# Create a device map to explicitly set max memory usage on devices. Avoid GPU out of memory by subtracting the memory used by CUDA cores above.\n",
    "# try adding dtype='float16' to reduce memory usage (but check performance!)\n",
    "\n",
    "device_map = infer_auto_device_map(\n",
    "    model, \n",
    "    max_memory={0: \"11GiB\", \"cpu\":\"30GiB\"}\n",
    ")\n",
    "\n",
    "# if you get Out Of Memory error in the following steps (or when inferencing), try to explicitly move one or several layers from the GPU to CPU/disk.\n",
    "import json\n",
    "\n",
    "#device_map[\"transformer.h.9\"] = \"cpu\" \n",
    "#device_map[\"transformer.h.10\"] = \"cpu\" \n",
    "\n",
    "#print(json.dumps(device_map, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650d1f76-81f4-4849-bcef-fc5e017a2b38",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from accelerate import load_checkpoint_and_dispatch\n",
    "\n",
    "model = load_checkpoint_and_dispatch(model, \"gpt-sw3-6.7b-v2\", device_map=device_map, offload_folder='offload', offload_state_dict=True) #create a folder named 'offload' before running\n",
    "#model.hf_device_map # print the device map to check what is loaded on different devices (GPUs, CPU and disk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca0a53f8-6f91-442f-ac6f-2e3cac3ab0e4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained('gpt-sw3-6.7b-v2', use_fast=True)\n",
    "\n",
    "prompt = \"\"\"\"\n",
    "<|endoftext|><s>\n",
    "User:\n",
    "Vad är sentimentet i följande mening? Den nya hemsidan är ganska snygg\n",
    "<s>\n",
    "Bot:\n",
    "\"\"\".strip()\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\").to(0)\n",
    "\n",
    "output = model.generate(\n",
    "    inputs=inputs[\"input_ids\"],\n",
    "    do_sample=True,\n",
    "    max_new_tokens=100,\n",
    "    temperature=0.75,\n",
    "    top_p=0.95,\n",
    "    repetition_penalty=1.2,\n",
    "    pad_token_id=tokenizer.eos_token_id\n",
    ")[0]\n",
    "\n",
    "generated_text = tokenizer.decode(output)  \n",
    "print(generated_text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
